{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install plotting libraries\n",
    "!pip install pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json\n",
    "import glob\n",
    "import concurrent.futures\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r') as f:\n",
    "            dictionary = json.load(f)\n",
    "        return dictionary\n",
    "    except:\n",
    "        print(\"bad filepath: \", filepath)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_plot(df, metric, datasets=None, sparsities=None, volume=None, network=None):\n",
    "\n",
    "    dataset_color_map = {\n",
    "        'cremi_c': '#C3186C',\n",
    "        'cremi_b': '#FE6100',\n",
    "        'cremi_a': '#FFC107',\n",
    "        'voljo': '#5E94F3',\n",
    "        'fib25': '#785EF0',\n",
    "        'epi': '#0FCEAD',\n",
    "    }\n",
    "\n",
    "    # filter df\n",
    "    plot_df = df.copy()\n",
    "    if datasets is not None:\n",
    "        plot_df = plot_df[plot_df['dataset'].isin(datasets)]\n",
    "    if sparsities is not None:\n",
    "        plot_df = plot_df[plot_df['sparsity'].isin(sparsities)]\n",
    "    if volume is not None:\n",
    "        plot_df = plot_df[plot_df['volume'].isin(volume)]\n",
    "    if network is not None:\n",
    "        plot_df = plot_df[plot_df['network'].isin(network)]\n",
    "\n",
    "    # make sub plots, one for each dataset\n",
    "    datasets = plot_df['dataset'].unique()\n",
    "    sparsities = sorted(plot_df['sparsity'].unique(), reverse=True)\n",
    "    sparsity_to_pos = {str(s): i for i, s in enumerate(sparsities)}\n",
    "    fig = plt.figure(figsize=(1.5*len(datasets) + 2, 4), dpi=300)\n",
    "    gs = matplotlib.gridspec.GridSpec(1, len(datasets) + 1, width_ratios=[1]*len(datasets) + [0.2])\n",
    "    axes = []\n",
    "\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        ax = fig.add_subplot(gs[0, idx])\n",
    "        axes.append(ax)\n",
    "\n",
    "        dataset_df = plot_df[plot_df['dataset'] == dataset]\n",
    "        dataset_df = dataset_df.sort_values(by='sparsity')\n",
    "\n",
    "        # plot metric on x axis, sparsity on y axis (more sparse is lower, more dense is higher)\n",
    "        y_positions = []\n",
    "        x_values = []\n",
    "        x_errors = []\n",
    "\n",
    "        for i, (index, row) in enumerate(dataset_df.iterrows()):\n",
    "            # Calculate a deterministic offset based on the index\n",
    "            offset = (i % 3 - 1) * 0.15  # Alternates between -0.1 and 0.1\n",
    "            y_positions.append(sparsity_to_pos[str(row['sparsity']).strip().split('\\n')[0]] + offset)\n",
    "\n",
    "        if isinstance(dataset_df[metric].iloc[0], pd.Series):\n",
    "            x_values = dataset_df[metric]['mean']\n",
    "            x_errors = dataset_df[metric]['std']\n",
    "            x_min = dataset_df[metric]['mean'].min() - dataset_df[metric]['std'].max()\n",
    "            x_max = dataset_df[metric]['mean'].max() + dataset_df[metric]['std'].min()\n",
    "\n",
    "        else:\n",
    "            x_values = dataset_df[metric]\n",
    "            x_errors = None\n",
    "            x_min = dataset_df[metric].min()\n",
    "            x_max = dataset_df[metric].max()\n",
    "\n",
    "        scatter = ax.scatter(x_values, y_positions, \n",
    "                           color=dataset_color_map[dataset],\n",
    "                           label=dataset, s=50, edgecolor='white', linewidth=0.5)\n",
    "        \n",
    "        if x_errors is not None:\n",
    "            ax.errorbar(x_values, y_positions,\n",
    "                        xerr=x_errors,\n",
    "                        fmt='none',\n",
    "                        color=dataset_color_map[dataset],\n",
    "                        alpha=0.5)\n",
    "\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, linestyle='--', linewidth=0.5, color='lightgray', axis='x')\n",
    "        ax.set_axisbelow(True)\n",
    "        ax.spines['top'].set_visible(True)\n",
    "        ax.spines['right'].set_visible(True)\n",
    "        \n",
    "        # axes labels\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Amount of human annotation')    \n",
    "            ax.set_yticks(range(len(sparsities)))\n",
    "            ax.set_yticklabels(list(sparsities))\n",
    "        else:   \n",
    "            ax.set_yticks(range(len(sparsities)))\n",
    "            ax.set_yticklabels(['' for _ in range(len(sparsities))])\n",
    "\n",
    "        ax.set_xlabel('')\n",
    "\n",
    "        # set x axis limits\n",
    "        x_margin = (x_max - x_min) * 0.2 # Add a margin of 10\n",
    "        ax.set_xlim([x_min - x_margin, x_max + x_margin])\n",
    "\n",
    "        # hide legend\n",
    "        #ax.legend().set_visible(False)\n",
    "\n",
    "    # axes, labels, title, legend, layout\n",
    "    fig.text(0.45, 0.02, metric, ha='center', va='center')\n",
    "    legend_ax = fig.add_subplot(gs[0, -1])\n",
    "    legend_ax.axis('off')\n",
    "    legend = legend_ax.legend(handles=[plt.scatter([], [], color=color, label=dataset) \n",
    "                                     for dataset, color in dataset_color_map.items()],\n",
    "                             loc='center left')\n",
    "    plt.tight_layout(rect=[0, 0.05, 0.95, 1])\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load result jsons recursively\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    gt_results = list(executor.map(load_file, glob.glob('**/results_gt*.json', recursive=True)))\n",
    "    pred_results = list(executor.map(load_file, glob.glob('**/results_pred*.json', recursive=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten pred results\n",
    "pred_data = []\n",
    "for result_dict in pred_results:\n",
    "    # each result file contains all the results for all the segs for a given pred\n",
    "    for r in result_dict.values():\n",
    "        result = {}\n",
    "        # cremi_c_rep1/dense/volume_1.zarr/3Af2M/5000--from--2d_mtlsd_3ch_20000/segmentations_ws/hist_quant_75--0.25--0.0001--1_2_2--xyTrue--minseed10\n",
    "        experiment_name, round_name, volume, setup_name, iteration_str, seg_prefix, seg_name = r['seg_ds'].split('/')[-7:]\n",
    "        result['dataset'] = experiment_name.split('_rep_')[0]\n",
    "        result['rep'] = int(experiment_name.split('_rep_')[1])\n",
    "        result['sparsity'] = round_name\n",
    "        result['volume'] = volume.split('.zarr')[0]\n",
    "        result['network'] = setup_name\n",
    "        result['iteration'] = int(iteration_str.split('--from--')[0])\n",
    "        result['prev_preds'] = None if len(iteration_str.split('--from--')) == 1 else iteration_str.split(str(result['iteration'])+'--from--')[1]\n",
    "        result['seg_method'] = seg_prefix.split('segmentations_')[1]\n",
    "        result['seg_name'] = seg_name\n",
    "        \n",
    "        for metric, value in r['error_map'].items():\n",
    "            result['errmap_'+metric] = value\n",
    "\n",
    "        for metric, value in r['error_mask'].items():\n",
    "            result['errmask_'+metric] = value\n",
    "\n",
    "        # seg params\n",
    "        if result['seg_method'] == 'ws':\n",
    "            continue # include only mws results\n",
    "            # seg_params = seg_name.split('--')\n",
    "            # merge_fn, threshold, min_seed = seg_params[0], seg_params[1], seg_params[-1]\n",
    "            # if len(seg_params) == 6:\n",
    "            #     noise_eps = seg_params[2]\n",
    "            #     sigma = seg_params[3]\n",
    "            # elif len(seg_params) == 5:\n",
    "            #     if '_' in seg_params[2]:\n",
    "            #         sigma = seg_params[2]\n",
    "            #         noise_eps = 0\n",
    "            #     else:\n",
    "            #         sigma = \"None\"\n",
    "            #         noise_eps = seg_params[2]\n",
    "\n",
    "            # result['merge_fn'] = merge_fn\n",
    "            # result['threshold'] = float(threshold)\n",
    "            # result['min_seed'] = min_seed\n",
    "            # result['noise_eps'] = float(noise_eps)\n",
    "            # result['sigma'] = sigma\n",
    "\n",
    "        if result['seg_method'] == 'mws':\n",
    "            seg_params = seg_name.split('--')\n",
    "            bias, rm_debris = seg_params[-2:]\n",
    "            if len(seg_params) == 3:\n",
    "                _, bias, rm_debris = seg_params\n",
    "                if '_' in seg_params[0]:\n",
    "                    sigma = seg_params[0]\n",
    "                    noise_eps = 0\n",
    "                else:\n",
    "                    sigma = \"None\"\n",
    "                    noise_eps = seg_params[0]\n",
    "            else:\n",
    "                noise_eps = seg_params[0]\n",
    "                sigma = seg_params[1]\n",
    "\n",
    "            result['bias'] = bias\n",
    "            result['short_bias']  = float(bias.split('_')[0].split('b')[-1])\n",
    "            result['long_bias']  = float(bias.split('_')[-1])\n",
    "            result['rm_debris'] = rm_debris\n",
    "            result['noise_eps'] = float(noise_eps)\n",
    "            result['sigma'] = sigma\n",
    "\n",
    "        pred_data.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten gt results\n",
    "gt_data = []\n",
    "for result_dict in gt_results:\n",
    "    # each result file contains all the results for all the segs for a given pred\n",
    "    for r in result_dict.values():\n",
    "        result = {}\n",
    "        # cremi_c_rep1/dense/volume_1.zarr/3Af2M/5000--from--2d_mtlsd_3ch_20000/segmentations_ws/hist_quant_75--0.25--0.0001--1_2_2--xyTrue--minseed10\n",
    "        experiment_name, round_name, volume, setup_name, iteration_str, seg_prefix, seg_name = r['seg_ds'].split('/')[-7:]\n",
    "        result['dataset'] = experiment_name.split('_rep_')[0]\n",
    "        result['rep'] = int(experiment_name.split('_rep_')[1])\n",
    "        result['sparsity'] = round_name\n",
    "        result['volume'] = volume.split('.zarr')[0]\n",
    "        result['network'] = setup_name\n",
    "        result['iteration'] = int(iteration_str.split('--from--')[0])\n",
    "        result['prev_preds'] = None if len(iteration_str.split('--from--')) == 1 else iteration_str.split(str(result['iteration'])+'--from--')[1]\n",
    "        result['seg_method'] = seg_prefix.split('segmentations_')[1]\n",
    "        result['seg_name'] = seg_name\n",
    "        for metric, value in (r['metrics']['voi'] | r['metrics']['skel']).items():\n",
    "            result[metric] = value\n",
    "\n",
    "        # more metrics\n",
    "        result['nvi_sum'] = result['nvi_split'] + result['nvi_merge']\n",
    "        result['voi_sum'] = result['voi_split'] + result['voi_merge']\n",
    "        result['total_skel_errors'] = result['n_splits'] + result['n_mergers']\n",
    "        result['errors_per_skel_length'] = 1000 * result['total_skel_errors'] / result['total_path_length']\n",
    "\n",
    "        # seg params\n",
    "        if result['seg_method'] == 'ws':\n",
    "            continue # include only mws\n",
    "            # seg_params = seg_name.split('--')\n",
    "            # merge_fn, threshold, min_seed = seg_params[0], seg_params[1], seg_params[-1]\n",
    "            # if len(seg_params) == 6:\n",
    "            #     noise_eps = seg_params[2]\n",
    "            #     sigma = seg_params[3]\n",
    "            # elif len(seg_params) == 5:\n",
    "            #     if '_' in seg_params[2]:\n",
    "            #         sigma = seg_params[2]\n",
    "            #         noise_eps = 0\n",
    "            #     else:\n",
    "            #         sigma = \"None\"\n",
    "            #         noise_eps = seg_params[2]\n",
    "\n",
    "            # result['merge_fn'] = merge_fn\n",
    "            # result['threshold'] = float(threshold)\n",
    "            # result['min_seed'] = min_seed\n",
    "            # result['noise_eps'] = float(noise_eps)\n",
    "            # result['sigma'] = sigma\n",
    "\n",
    "        if result['seg_method'] == 'mws':\n",
    "            seg_params = seg_name.split('--')\n",
    "            bias, rm_debris = seg_params[-2:]\n",
    "            if len(seg_params) == 3:\n",
    "                _, bias, rm_debris = seg_params\n",
    "                if '_' in seg_params[0]:\n",
    "                    sigma = seg_params[0]\n",
    "                    noise_eps = 0\n",
    "                else:\n",
    "                    sigma = \"None\"\n",
    "                    noise_eps = seg_params[0]\n",
    "            else:\n",
    "                noise_eps = seg_params[0]\n",
    "                sigma = seg_params[1]\n",
    "\n",
    "            result['bias'] = bias\n",
    "            result['short_bias']  = float(bias.split('_')[0].split('b')[-1])\n",
    "            result['long_bias']  = float(bias.split('_')[-1])\n",
    "            result['rm_debris'] = rm_debris\n",
    "            result['noise_eps'] = float(noise_eps)\n",
    "            result['sigma'] = sigma\n",
    "\n",
    "        gt_data.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframes\n",
    "# then, merge gt and pred dataframes by grouping dataset, rep, sparsity, volume, iteration, prev_pred, seg_method, and seg_name\n",
    "# basically we want to merge the gt and pred dataframes by the columns that are the same, and then include the columns that are different\n",
    "# i.e, one dataframe for all datasets, sparsities, reps, volumes, with gt and pred results.\n",
    "# if a row does not exist in either df, then we will add as in the other df with NaNs for the columns which dont have a value\n",
    "\n",
    "gt_df = pd.DataFrame(gt_data)\n",
    "pred_df = pd.DataFrame(pred_data)\n",
    "all_results = pd.merge(gt_df, pred_df, on=['dataset', 'sparsity', 'rep', 'network', 'volume', 'iteration', 'prev_preds', 'seg_method', 'seg_name', 'bias', 'short_bias', 'long_bias', 'rm_debris', 'noise_eps', 'sigma'], how='outer')\n",
    "# nan_df = all_results[all_results.isna().any(axis=1)] # get all rows with nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce merged df\n",
    "group_by = ['dataset', 'rep', 'sparsity', 'volume', 'network',]\n",
    "#group_by += ['iteration', 'prev_preds']\n",
    "\n",
    "metric_for_best_gt = 'nvi_sum'\n",
    "metric_for_best_self = 'errmask_mean'\n",
    "metrics_to_avg = ['rand_split', 'rand_merge', 'voi_split', 'voi_merge', 'nvi_split', 'nvi_merge', 'nid', 'erl', 'nerl', 'n_mergers', 'n_splits', 'n_non0_mergers', 'nvi_sum', 'voi_sum', 'total_skel_errors', 'errors_per_skel_length', 'errmap_mean', 'errmap_std', 'errmask_mean', 'errmask_std', 'errmask_num_nonzero_voxels', 'errmap_num_nonzero_voxels', 'errmask_nonzero_ratio', 'errmap_nonzero_ratio']\n",
    "\n",
    "# 'best' reduction: pick best segmentation result for each combination of columns in `group_by`\n",
    "best_results_gt = all_results.loc[all_results.groupby(group_by)[metric_for_best_gt].idxmin()].reset_index()\n",
    "best_results_self = all_results.loc[all_results.groupby(group_by)[metric_for_best_self].idxmin()].reset_index()\n",
    "\n",
    "# 'avg' reduction: average over all segmentation results for each combination of columns in `group_by`, with mean and std dev\n",
    "avg_results = all_results.groupby(group_by).agg({col: ['mean', 'std'] for col in metrics_to_avg}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_plot(best_results_self, 'errmask_mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_plot(avg_results, 'errmap_mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_columns = best_results_gt.select_dtypes(exclude=['float']).columns\n",
    "diff_df = (best_results_gt.select_dtypes(include=['float']) - best_results_self.select_dtypes(include=['float']))/best_results_gt.select_dtypes(include=['float'])\n",
    "diff_df = pd.concat([best_results_gt[meta_columns], diff_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset_plot(diff_df, 'errors_per_skel_length');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export best segmentations\n",
    "best_segs_gt_eval = best_results_gt.to_dict('index')\n",
    "best_segs_self_eval = best_results_self.to_dict('index')\n",
    "\n",
    "# save to json\n",
    "with open('round_1_best_segs_gt_eval.json', 'w') as f:\n",
    "    json.dump(best_segs_gt_eval, f, indent=4)\n",
    "with open('round_1_best_segs_self_eval.json', 'w') as f:\n",
    "    json.dump(best_segs_self_eval, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
